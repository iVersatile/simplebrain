# SimpleBrain Architecture

## Overview

SimpleBrain is a self-contained, lightweight local AI assistant system designed for offline operation with minimal dependencies. It provides a complete AI inference stack using llama.cpp for local model execution and a Flask-based REST API for interaction.

## ğŸ—ï¸ System Architecture

### High-Level Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SimpleBrain System                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CLI Interfaces â”‚  â”‚   Flask API     â”‚  â”‚  Local Models   â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚ â€¢ ask_agent.sh  â”‚  â”‚ â€¢ REST API      â”‚  â”‚ â€¢ Phi-3 Mini    â”‚  â”‚
â”‚  â”‚ â€¢ cli_agent.py  â”‚  â”‚ â€¢ Port 5001     â”‚  â”‚ â€¢ Mistral 7B    â”‚  â”‚
â”‚  â”‚ â€¢ llama_direct  â”‚  â”‚ â€¢ JSON I/O      â”‚  â”‚ â€¢ Llama 3 8B    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                     â”‚                     â”‚         â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Docker Container                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚    Flask    â”‚  â”‚ llama.cpp   â”‚  â”‚    Security Layer   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  Web Server â”‚  â”‚  Inference  â”‚  â”‚                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚             â”‚  â”‚   Engine    â”‚  â”‚ â€¢ Non-root user     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ HTTP API  â”‚  â”‚             â”‚  â”‚ â€¢ Read-only FS      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ JSON      â”‚  â”‚ â€¢ GGUF      â”‚  â”‚ â€¢ Resource limits   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Routing   â”‚  â”‚ â€¢ CPU only  â”‚  â”‚ â€¢ Network isolation â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Input â†’ CLI/API â†’ Flask Router â†’ LLM Interface â†’ llama.cpp â†’ Model
    â†“                                                                 â†“
Response â† JSON API â† Response Parser â† Command Executor â† LLM Output
```

## ğŸ§© Component Details

### 1. Container Infrastructure

**Base Image**: Ubuntu 22.04
- **Purpose**: Provides stable foundation with all required dependencies
- **Security**: Hardened with non-root user, read-only filesystem, resource limits
- **Isolation**: Custom bridge network with restricted inter-container communication

### 2. AI Inference Engine

**llama.cpp**
- **Purpose**: High-performance CPU-based LLM inference
- **Models**: Supports GGUF format quantized models
- **Features**: Multi-threading, memory mapping, optimized kernels
- **Integration**: Called via Python subprocess interface

### 3. Application Layer

**Flask Web Server**
- **Purpose**: HTTP API for AI interactions
- **Port**: 5001 (external) â†’ 5000 (internal)
- **Endpoints**: `/api/agent` (POST)
- **Format**: JSON request/response

**Python Components**:
- `app.py`: Main Flask application
- `llm_interface.py`: LLM communication layer
- `agent_actions.py`: Command execution (disabled for security)
- `cli.py`: Command-line interface

### 4. Model Management

**Multi-Model Support**:
- Current model: `models/model.gguf` (symlink)
- Available models: `phi3-mini-4k.gguf`, `mistral-7b.gguf`, `llama3-8b.gguf`
- Switching: Automated via `switch_model.sh`

### 5. CLI Interfaces

**User-Facing Scripts**:
- `ask_agent.sh`: Single-question interface
- `cli_agent.py`: Interactive chat session
- `llama_direct.sh`: Direct model access

## ğŸ”§ Technical Specifications

### System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| RAM | 4GB | 8GB+ |
| CPU | 2 cores | 4+ cores |
| Storage | 6GB | 10GB+ |
| Network | None (offline) | Optional |

### Model Specifications

| Model | Size | RAM Usage | Context | Speed |
|-------|------|-----------|---------|-------|
| Phi-3 Mini | 2.4GB | 4GB | 4K tokens | Fast |
| Mistral 7B | 4.1GB | 6GB | 8K tokens | Medium |
| Llama 3 8B | 4.6GB | 8GB | 8K tokens | Slower |

### Security Architecture

**Container Security**:
- Non-root user (uid: 1000)
- Read-only root filesystem
- Dropped capabilities (CAP_DROP: ALL)
- Resource limits (CPU: 4 cores, RAM: 6GB)
- Network isolation (custom bridge)

**Data Security**:
- Model files: Read-only mounts
- Workspace: Restricted write access
- Temporary files: tmpfs with size limits
- Logging: Size-limited rotation

## ğŸ”„ Operational Flow

### Startup Sequence

1. **Container Initialization**
   - Load Ubuntu 22.04 base image
   - Install dependencies (Python, build tools)
   - Create non-root user environment

2. **Service Preparation**
   - Mount volume directories
   - Verify model file existence
   - Initialize Flask application

3. **Runtime State**
   - Flask server listening on port 5000
   - llama.cpp ready for inference
   - CLI tools available for interaction

### Request Processing

1. **Input Reception**
   - CLI tool or HTTP request
   - JSON payload with prompt

2. **LLM Processing**
   - Prompt formatting and context addition
   - llama.cpp subprocess execution
   - Response generation with token limits

3. **Response Handling**
   - Command parsing (if applicable)
   - Security validation
   - JSON response formatting

## ğŸ“¦ File Structure

```
simplebrain/
â”œâ”€â”€ Dockerfile.local-llm           # Container definition
â”œâ”€â”€ docker-compose.local-llm.yml   # Orchestration config
â”œâ”€â”€ automate_local_llm.sh          # Main automation script
â”œâ”€â”€ setup_local_llm.sh             # One-time setup
â”œâ”€â”€ health_check_llm.sh            # Health monitoring
â”œâ”€â”€ switch_model.sh                # Model switching
â”œâ”€â”€ ask_agent.sh                   # Quick CLI interface
â”œâ”€â”€ cli_agent.py                   # Interactive interface
â”œâ”€â”€ llama_direct.sh                # Direct model access
â”œâ”€â”€ models/                        # AI model files
â”‚   â”œâ”€â”€ model.gguf                 # Current model (symlink)
â”‚   â”œâ”€â”€ phi3-mini-4k.gguf          # Phi-3 Mini model
â”‚   â”œâ”€â”€ mistral-7b.gguf            # Mistral 7B model
â”‚   â””â”€â”€ llama3-8b.gguf             # Llama 3 8B model
â”œâ”€â”€ workspace/                     # llama.cpp build
â”‚   â”œâ”€â”€ llama.cpp/                 # Source code
â”‚   â””â”€â”€ llama.cpp.tar.gz           # Source archive
â”œâ”€â”€ local_agent_workspace/         # Flask application
â”‚   â”œâ”€â”€ app.py                     # Main Flask app
â”‚   â”œâ”€â”€ llm_interface.py           # LLM communication
â”‚   â”œâ”€â”€ agent_actions.py           # Command handling
â”‚   â”œâ”€â”€ cli.py                     # CLI interface
â”‚   â””â”€â”€ run_app.sh                 # Application launcher
â”œâ”€â”€ README.md                      # User documentation
â”œâ”€â”€ SIMPLIFIED_SYSTEM.md           # System details
â””â”€â”€ ARCHITECTURE.md                # This file
```

## ğŸ” Security Considerations

### Container Security

**Read-only Filesystem**:
- Root filesystem mounted read-only
- Writable areas limited to tmpfs
- Prevents persistence of malicious changes

**Resource Limits**:
- CPU: 4 cores maximum
- Memory: 6GB maximum
- PIDs: 300 maximum
- Prevents resource exhaustion attacks

**Network Isolation**:
- Custom bridge network
- Inter-container communication disabled
- Limited to necessary port exposure

### Application Security

**Command Execution**:
- Disabled by default for security
- Input validation and sanitization
- Restricted command whitelist (if enabled)

**Model Security**:
- Read-only model file access
- Validated model formats (GGUF)
- No external model downloads in runtime

## ğŸš€ Deployment Architecture

### Single-Host Deployment

```
Host System (macOS/Linux)
â”œâ”€â”€ Docker Engine
â””â”€â”€ SimpleBrain Container
    â”œâ”€â”€ Flask API Server
    â”œâ”€â”€ llama.cpp Engine
    â””â”€â”€ Model Files
```

### Development Workflow

1. **Setup**: Run `setup_local_llm.sh`
2. **Development**: Use `automate_local_llm.sh start`
3. **Testing**: Access via CLI tools or HTTP API
4. **Monitoring**: Check health with `health_check_llm.sh`
5. **Shutdown**: Use `automate_local_llm.sh stop`

## ğŸ”§ Configuration Management

### Environment Variables

```bash
PYTHONPATH=/app                    # Python module path
MODEL_PATH=/app/models/model.gguf  # Current model path
FLASK_ENV=development              # Flask environment
FLASK_APP=app.py                   # Flask application
```

### Volume Mounts

```yaml
./workspace â†’ /app/workspace        # llama.cpp build (RW)
./models â†’ /app/models              # Model files (RO)
./local_agent_workspace â†’ /app/local_agent  # Flask app (RW)
```

## ğŸ“Š Performance Characteristics

### Inference Performance

- **Phi-3 Mini**: ~20-30 tokens/second
- **Mistral 7B**: ~10-15 tokens/second
- **Llama 3 8B**: ~8-12 tokens/second

### Memory Usage

- **Base container**: ~500MB
- **Plus model**: +2-5GB depending on model
- **Peak usage**: Model size + 1-2GB buffer

### Startup Time

- **Container start**: ~5-10 seconds
- **Model loading**: ~10-30 seconds
- **First inference**: ~2-5 seconds

## ğŸ¯ Design Principles

### Simplicity
- Minimal dependencies
- Single-purpose components
- Clear separation of concerns

### Security
- Defense in depth
- Least privilege access
- Isolated execution environment

### Performance
- CPU-optimized inference
- Memory-efficient models
- Fast startup and response

### Maintainability
- Modular architecture
- Clear interfaces
- Comprehensive logging

---

**SimpleBrain Architecture** - Designed for simplicity, security, and performance! ğŸ§ âœ¨